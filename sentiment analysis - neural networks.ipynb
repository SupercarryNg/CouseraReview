{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2453148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c96db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f7c3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107013</th>\n",
       "      <td>107013</td>\n",
       "      <td>Trendy topic with talks from expertises in the...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107014</th>\n",
       "      <td>107014</td>\n",
       "      <td>Wonderful! Simple and clear language, good ins...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107015</th>\n",
       "      <td>107015</td>\n",
       "      <td>an interesting and fun course. thanks. dr quincy</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107016</th>\n",
       "      <td>107016</td>\n",
       "      <td>very broad perspective, up to date information...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107017</th>\n",
       "      <td>107017</td>\n",
       "      <td>An informative course on the social and financ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                             Review  Label  y\n",
       "107013  107013  Trendy topic with talks from expertises in the...      4  2\n",
       "107014  107014  Wonderful! Simple and clear language, good ins...      5  2\n",
       "107015  107015   an interesting and fun course. thanks. dr quincy      5  2\n",
       "107016  107016  very broad perspective, up to date information...      4  2\n",
       "107017  107017  An informative course on the social and financ...      4  2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_original.copy()\n",
    "df['y'] = df.Label.replace({1: 0, 2: 0, 3: 1, 4: 2, 5: 2})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c125a32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    97227\n",
       "1     5071\n",
       "0     4720\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cad17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def preprocessingText(corpus, lowercase=True, rmPunctuation=True, rpURL=True, rpNumber=True, stemming=True):\n",
    "    \"\"\"Input is assumed to be vector of documents\"\"\"\n",
    "    documents = []\n",
    "    for text in corpus:\n",
    "        document = text\n",
    "        \n",
    "        # HYPERPARAMETER\n",
    "        # Converting to Lowercase\n",
    "        if lowercase:\n",
    "            document = document.lower()\n",
    "\n",
    "        # replace URL\n",
    "        if rpURL:\n",
    "            # replace URL\n",
    "            document = re.sub(r'http\\S+', 'url', document, flags=re.MULTILINE)\n",
    "\n",
    "        # replace numbers\n",
    "        if rpNumber:\n",
    "            document = re.sub(\"\\d+\", \"number\", document)\n",
    "\n",
    "        # remove all special characters including punctuation\n",
    "        if rmPunctuation:\n",
    "            # only keep word\n",
    "            document = re.sub(r'\\W', ' ', document)\n",
    "            # remove all single characters\n",
    "            document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "            # Remove single characters from the start\n",
    "            document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # OTHER PREPROCESSING METHODS\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "        \n",
    "        # removing stopwords\n",
    "        document = document.split()\n",
    "        document = [word for word in document if word not in STOPWORDS]\n",
    "\n",
    "        if stemming:\n",
    "            # Lemmatization\n",
    "            document = [stemmer.lemmatize(word) for word in document]\n",
    "            # stemming\n",
    "            document = [porter.stem(word) for word in document]\n",
    "\n",
    "        document = ' '.join(document)\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd088de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingText_not_moving_stop_words(corpus, lowercase=True, rmPunctuation=True, rpURL=True, rpNumber=True, stemming=True):\n",
    "    \"\"\"Input is assumed to be vector of documents\"\"\"\n",
    "    documents = []\n",
    "    for text in corpus:\n",
    "        document = text\n",
    "        \n",
    "        # HYPERPARAMETER\n",
    "        # Converting to Lowercase\n",
    "        if lowercase:\n",
    "            document = document.lower()\n",
    "\n",
    "        # replace URL\n",
    "        if rpURL:\n",
    "            # replace URL\n",
    "            document = re.sub(r'http\\S+', 'url', document, flags=re.MULTILINE)\n",
    "\n",
    "        # replace numbers\n",
    "        if rpNumber:\n",
    "            document = re.sub(\"\\d+\", \"number\", document)\n",
    "\n",
    "        # remove all special characters including punctuation\n",
    "        if rmPunctuation:\n",
    "            # only keep word\n",
    "            document = re.sub(r'\\W', ' ', document)\n",
    "            # remove all single characters\n",
    "            document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "            # Remove single characters from the start\n",
    "            document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # OTHER PREPROCESSING METHODS\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "        \n",
    "        # removing stopwords\n",
    "        document = document.split()\n",
    "        document = [word for word in document if word not in STOPWORDS]\n",
    "\n",
    "        if stemming:\n",
    "            # Lemmatization\n",
    "            document = [stemmer.lemmatize(word) for word in document]\n",
    "            # stemming\n",
    "            document = [porter.stem(word) for word in document]\n",
    "\n",
    "        document = ' '.join(document)\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7180334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107013    trendi topic talk expertis field cover area in...\n",
      "107014    wonder simpl clear languag good instructor gre...\n",
      "107015                   interest fun cours thank dr quinci\n",
      "107016    broad perspect date inform use link video good...\n",
      "107017    inform cours social financi implic due zika we...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['text'] = preprocessingText(df.Review)\n",
    "print(df.tail().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3ad4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>67348</td>\n",
       "      <td>The course is very interesting. In the beginni...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>cours interest begin seem bit heavi neurosci i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38747</th>\n",
       "      <td>38747</td>\n",
       "      <td>Considering that internet and media regulation...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>consid internet medium regul hot topic polici ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17832</th>\n",
       "      <td>17832</td>\n",
       "      <td>Quite useful for introducing Data Science. I w...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>quit use introduc data scienc enrol data scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65168</th>\n",
       "      <td>65168</td>\n",
       "      <td>Great course!This course is perfect for beginn...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>great cours cours perfect beginn interest lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46273</th>\n",
       "      <td>46273</td>\n",
       "      <td>Very useful and effective</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>use effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83880</th>\n",
       "      <td>83880</td>\n",
       "      <td>It's a very good course. I learn more about th...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>good cours learn lesson thank teacher lesson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30829</th>\n",
       "      <td>30829</td>\n",
       "      <td>This is a really good start.</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>realli good start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>8588</td>\n",
       "      <td>good for basic knowledge in cancer biology.</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>good basic knowledg cancer biolog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91826</th>\n",
       "      <td>91826</td>\n",
       "      <td>Fantastic course as always from the legendary ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>fantast cours alway legendari dr chuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23564</th>\n",
       "      <td>23564</td>\n",
       "      <td>Love it!!! Very educative and well resourced.</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>love educ well resourc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5071 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id                                             Review  Label  y  \\\n",
       "67348  67348  The course is very interesting. In the beginni...      5  2   \n",
       "38747  38747  Considering that internet and media regulation...      5  2   \n",
       "17832  17832  Quite useful for introducing Data Science. I w...      5  2   \n",
       "65168  65168  Great course!This course is perfect for beginn...      5  2   \n",
       "46273  46273                          Very useful and effective      4  2   \n",
       "...      ...                                                ...    ... ..   \n",
       "83880  83880  It's a very good course. I learn more about th...      5  2   \n",
       "30829  30829                       This is a really good start.      5  2   \n",
       "8588    8588        good for basic knowledge in cancer biology.      5  2   \n",
       "91826  91826  Fantastic course as always from the legendary ...      5  2   \n",
       "23564  23564      Love it!!! Very educative and well resourced.      5  2   \n",
       "\n",
       "                                                    text  \n",
       "67348  cours interest begin seem bit heavi neurosci i...  \n",
       "38747  consid internet medium regul hot topic polici ...  \n",
       "17832  quit use introduc data scienc enrol data scien...  \n",
       "65168  great cours cours perfect beginn interest lear...  \n",
       "46273                                         use effect  \n",
       "...                                                  ...  \n",
       "83880       good cours learn lesson thank teacher lesson  \n",
       "30829                                  realli good start  \n",
       "8588                   good basic knowledg cancer biolog  \n",
       "91826             fantast cours alway legendari dr chuck  \n",
       "23564                             love educ well resourc  \n",
       "\n",
       "[5071 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(8307)\n",
    "positive_indices = df[df.y == 2].index\n",
    "random_indices = np.random.choice(positive_indices, 5071, replace=False)\n",
    "positive_sample = df.loc[random_indices]\n",
    "positive_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff0198c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    5071\n",
       "1    5071\n",
       "0    4720\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([positive_sample, df[df['y'] != 2]], verify_integrity=True)\n",
    "df2['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a327f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2[\"text\"], list(df2['y']), \n",
    "                                                    test_size=0.1, \n",
    "                                                    stratify=df2['y'],\n",
    "                                                    random_state=8307)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8feb7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "X_train_token = []\n",
    "X_test_token = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfab83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in X_train:\n",
    "    words = word_tokenize(words)\n",
    "    X_train_token.append(words)\n",
    "\n",
    "for words in X_test:\n",
    "    words = word_tokenize(words)\n",
    "    X_test_token.append(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f533bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160e154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_token)\n",
    "word_index = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f440eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_data = tokenizer.texts_to_sequences(X_train_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf92284",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_data = tokenizer.texts_to_sequences(X_test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eadeec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(w) for w in encoded_train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1129a790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cd6d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(encoded_train_data, \n",
    "                        maxlen=max_length, \n",
    "                        padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "662aba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad = pad_sequences(encoded_test_data, \n",
    "                        maxlen=max_length, \n",
    "                        padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d795e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dummy = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "927380c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_dummy = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7705b",
   "metadata": {},
   "source": [
    "tf-idf embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72fb2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4939d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "X_train_tf = tfidf.transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c5f417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = X_train_tf.toarray()\n",
    "X_test_array = X_test_tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51dea09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13375, 9618)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d879b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict = tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0686ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdf_embedding_matrix = np.zeros((len(word_index)+1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daeb9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in word_index.items():\n",
    "    try:\n",
    "        vector = tfidf_dict[v]\n",
    "        tfdf_embedding_matrix[k] = vector\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f941ab14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "       [1807., 1807., 1807., ..., 1807., 1807., 1807.],\n",
       "       [5686., 5686., 5686., ..., 5686., 5686., 5686.],\n",
       "       ...,\n",
       "       [3508., 3508., 3508., ..., 3508., 3508., 3508.],\n",
       "       [2469., 2469., 2469., ..., 2469., 2469., 2469.],\n",
       "       [9551., 9551., 9551., ..., 9551., 9551., 9551.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bd6ee73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9645, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4679d2",
   "metadata": {},
   "source": [
    "word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "688b00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import torch\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf8ba856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(X_train_token,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcda42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a728b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vdict = {}\n",
    "for key in my_wv.index_to_key:\n",
    "    w2vdict[key] = my_wv[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20fbf44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3067"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2vdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e24a8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix\n",
    "embedding_matrix = np.zeros((len(word_index)+1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb25d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in word_index.items():\n",
    "    try:\n",
    "        vector = model.wv[v]\n",
    "        embedding_matrix[k] = vector\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8377b5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.00730391, -0.48694927, -0.16433622, ...,  1.07726753,\n",
       "         0.24238859,  0.92591852],\n",
       "       [ 0.04689787,  1.3033303 ,  1.31418991, ...,  0.20105635,\n",
       "        -1.60167646, -0.41524702],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4a4e0",
   "metadata": {},
   "source": [
    "## NN TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c10c63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e993113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDATA(Dataset):\n",
    "    def __init__(self,X,Y,feature_type = torch.int64,target_type =torch.int64):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        self.feature_type = feature_type\n",
    "        self.target_type = target_type\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx]\n",
    "        feature = torch.tensor(feature,dtype = self.feature_type)\n",
    "        target = self.y[idx]\n",
    "        target = torch.tensor(target,dtype=self.target_type)\n",
    "        \n",
    "        return feature,target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45d61f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_tf = NLPDATA(X_train_array,y_train,feature_type=torch.float32)\n",
    "valdata_tf = NLPDATA(X_test_array,y_test,feature_type=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff0b94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "545866bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim,out_features=hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7299575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "input_dim = 9618\n",
    "hidden_dim = 256\n",
    "output_dim = 3\n",
    "mynn = NormalNN(input_dim,hidden_dim,output_dim).to(device)\n",
    "optim = Adam(mynn.parameters(),lr=lr)\n",
    "loss = CrossEntropyLoss().to(device)\n",
    "train_loader_tf = DataLoader(traindata_tf, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "validate_loader_tf = DataLoader(valdata_tf, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b3f40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model,epochs,optim,loss,train_loader,validate_loader):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss= 0\n",
    "        validation_loss = 0 \n",
    "        total_correct = 0\n",
    "        for data in train_loader:\n",
    "            optim.zero_grad()\n",
    "            features, targets = data\n",
    "            features, targets = features.to(device),targets.to(device)\n",
    "            output = model(features)\n",
    "            result = loss(output,targets)\n",
    "            result.backward()\n",
    "            optim.step()\n",
    "            running_loss +=result\n",
    "        with torch.no_grad():\n",
    "            total_len = len(valdata_tf)\n",
    "            for data in validate_loader:\n",
    "                features,targets = data\n",
    "                features, targets = features.to(device),targets.to(device)\n",
    "                output = model(features)\n",
    "                result = loss(output,targets)\n",
    "                validation_loss += result\n",
    "                correct_num = sum(targets.eq(output.argmax(dim=1)))\n",
    "                total_correct+=correct_num\n",
    "        accuracy = total_correct/total_len   \n",
    "        print(\"epoch: \",epoch,\"running loss: \", running_loss.item(),\"accuracy: \",accuracy.item(),\"validation loss: \",validation_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3e93433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 running loss:  197.6351318359375 accuracy:  0.6267653107643127 validation loss:  21.87410545349121\n",
      "epoch:  1 running loss:  181.97772216796875 accuracy:  0.64492267370224 validation loss:  21.47779083251953\n",
      "epoch:  2 running loss:  171.2307891845703 accuracy:  0.6745124459266663 validation loss:  20.64545249938965\n",
      "epoch:  3 running loss:  162.8377227783203 accuracy:  0.6785473823547363 validation loss:  20.55609893798828\n",
      "epoch:  4 running loss:  157.66128540039062 accuracy:  0.6765299439430237 validation loss:  20.531936645507812\n",
      "epoch:  5 running loss:  153.61807250976562 accuracy:  0.6657699942588806 validation loss:  20.894866943359375\n",
      "epoch:  6 running loss:  150.20263671875 accuracy:  0.6597175598144531 validation loss:  20.85379409790039\n",
      "epoch:  7 running loss:  147.83615112304688 accuracy:  0.6637524962425232 validation loss:  20.86774444580078\n",
      "epoch:  8 running loss:  145.4949951171875 accuracy:  0.6597175598144531 validation loss:  21.12055206298828\n",
      "epoch:  9 running loss:  143.7605743408203 accuracy:  0.6536651253700256 validation loss:  21.12908172607422\n",
      "epoch:  10 running loss:  142.32968139648438 accuracy:  0.6516476273536682 validation loss:  21.461624145507812\n",
      "epoch:  11 running loss:  141.2161865234375 accuracy:  0.6482851505279541 validation loss:  21.240137100219727\n",
      "epoch:  12 running loss:  140.16873168945312 accuracy:  0.6509751081466675 validation loss:  21.256818771362305\n",
      "epoch:  13 running loss:  139.47325134277344 accuracy:  0.6516476273536682 validation loss:  21.483821868896484\n",
      "epoch:  14 running loss:  138.62806701660156 accuracy:  0.6476126313209534 validation loss:  21.390039443969727\n",
      "epoch:  15 running loss:  138.0345458984375 accuracy:  0.6402152180671692 validation loss:  21.311233520507812\n",
      "epoch:  16 running loss:  137.46031188964844 accuracy:  0.6523200869560242 validation loss:  21.428239822387695\n",
      "epoch:  17 running loss:  136.93177795410156 accuracy:  0.6503026485443115 validation loss:  21.451784133911133\n",
      "epoch:  18 running loss:  137.07901000976562 accuracy:  0.6469401717185974 validation loss:  21.65428924560547\n",
      "epoch:  19 running loss:  136.58074951171875 accuracy:  0.6462676525115967 validation loss:  21.481678009033203\n"
     ]
    }
   ],
   "source": [
    "main(mynn,20,optim,loss,train_loader_tf,validate_loader_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f29d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1487"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valdata_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0aa23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(valdata_tf, batch_size=len(valdata_tf), shuffle=False, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2aa75903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6463)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_len = len(valdata_tf)\n",
    "    for data in testloader:\n",
    "        features,targets = data\n",
    "        features, targets = features.to(device),targets.to(device)\n",
    "        output = mynn(features)\n",
    "        correct_num = sum(targets.eq(output.argmax(dim=1)))\n",
    "acc = correct_num/total_len\n",
    "acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a49cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = [\"negative\",\"neutual\",\"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69898316",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.argmax(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e420007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e35d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61       472\n",
      "           1       0.54      0.57      0.55       508\n",
      "           2       0.76      0.79      0.77       507\n",
      "\n",
      "    accuracy                           0.65      1487\n",
      "   macro avg       0.65      0.64      0.64      1487\n",
      "weighted avg       0.65      0.65      0.65      1487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68022981",
   "metadata": {},
   "source": [
    "## NN-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0a25f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNW2V(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim,weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        self.embedding.requires_grad_ = False        \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)\n",
    "        # pooled = [batch size, embedding dim]\n",
    "        prediction = self.fc(x)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94122c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_nn_w2v = NLPDATA(X_train_pad,y_train,feature_type=torch.int64)\n",
    "valdata_nn_w2v = NLPDATA(X_test_pad,y_test,feature_type=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df4651c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(embedding_matrix)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "embedding_dim = 100\n",
    "output_dim = 3\n",
    "mynnw2v = NNW2V(embedding_dim,output_dim,weights).to(device)\n",
    "optim = Adam(mynnw2v.parameters(),lr=lr)\n",
    "loss = CrossEntropyLoss().to(device)\n",
    "train_loader_nn_w2v = DataLoader(traindata_nn_w2v, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "validate_loader_nn_w2v = DataLoader(valdata_nn_w2v, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7493e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_nn_w2v = DataLoader(valdata_nn_w2v, batch_size=len(valdata_nn_w2v), shuffle=False, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f880a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 running loss:  168.89596557617188 accuracy:  0.6556825637817383 validation loss:  19.621353149414062\n",
      "epoch:  1 running loss:  168.81578063964844 accuracy:  0.6536651253700256 validation loss:  19.70125961303711\n",
      "epoch:  2 running loss:  168.74046325683594 accuracy:  0.6536651253700256 validation loss:  19.458599090576172\n",
      "epoch:  3 running loss:  168.66761779785156 accuracy:  0.6550101041793823 validation loss:  19.488645553588867\n",
      "epoch:  4 running loss:  168.5882110595703 accuracy:  0.6523200869560242 validation loss:  19.52179718017578\n",
      "epoch:  5 running loss:  168.51644897460938 accuracy:  0.6536651253700256 validation loss:  19.345821380615234\n",
      "epoch:  6 running loss:  168.44239807128906 accuracy:  0.6529926061630249 validation loss:  19.55341911315918\n",
      "epoch:  7 running loss:  168.37200927734375 accuracy:  0.6509751081466675 validation loss:  19.70565414428711\n",
      "epoch:  8 running loss:  168.30886840820312 accuracy:  0.6523200869560242 validation loss:  19.391353607177734\n",
      "epoch:  9 running loss:  168.23350524902344 accuracy:  0.6523200869560242 validation loss:  19.397062301635742\n",
      "epoch:  10 running loss:  168.16912841796875 accuracy:  0.6503026485443115 validation loss:  19.389089584350586\n",
      "epoch:  11 running loss:  168.10296630859375 accuracy:  0.6536651253700256 validation loss:  19.31515884399414\n",
      "epoch:  12 running loss:  168.03030395507812 accuracy:  0.6509751081466675 validation loss:  19.373315811157227\n",
      "epoch:  13 running loss:  167.9671630859375 accuracy:  0.6536651253700256 validation loss:  19.306793212890625\n",
      "epoch:  14 running loss:  167.90704345703125 accuracy:  0.6509751081466675 validation loss:  19.35976219177246\n",
      "epoch:  15 running loss:  167.84814453125 accuracy:  0.6523200869560242 validation loss:  19.345399856567383\n",
      "epoch:  16 running loss:  167.79354858398438 accuracy:  0.6516476273536682 validation loss:  19.293670654296875\n",
      "epoch:  17 running loss:  167.72349548339844 accuracy:  0.6516476273536682 validation loss:  19.762659072875977\n",
      "epoch:  18 running loss:  167.66432189941406 accuracy:  0.6523200869560242 validation loss:  19.38718032836914\n",
      "epoch:  19 running loss:  167.60989379882812 accuracy:  0.6543375849723816 validation loss:  19.374704360961914\n"
     ]
    }
   ],
   "source": [
    "main(mynnw2v,20,optim,loss,train_loader_nn_w2v,validate_loader_nn_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8dfde0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6543)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_len = len(valdata_nn_w2v)\n",
    "    for data in test_loader_nn_w2v:\n",
    "        features,targets = data\n",
    "        features, targets = features.to(device),targets.to(device)\n",
    "        output = mynnw2v(features)\n",
    "        correct_num = sum(targets.eq(output.argmax(dim=1)))\n",
    "acc = correct_num/total_len\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b4702d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.57      0.63       472\n",
      "           1       0.58      0.54      0.56       508\n",
      "           2       0.67      0.85      0.75       507\n",
      "\n",
      "    accuracy                           0.65      1487\n",
      "   macro avg       0.66      0.65      0.65      1487\n",
      "weighted avg       0.65      0.65      0.65      1487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = output.argmax(dim=1).numpy()\n",
    "print(classification_report(y_test,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba72d0f",
   "metadata": {},
   "source": [
    "# RNN W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a2dc9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d83d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                                     output_dim=embedding_matrix.shape[1], \n",
    "                                     weights=[embedding_matrix],\n",
    "                                     input_length=max_length, \n",
    "                                     trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f1af0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 128, 100)          964500    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128, 64)           31872     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,005,879\n",
      "Trainable params: 41,379\n",
      "Non-trainable params: 964,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the RNN model \n",
    "RNN_Model = Sequential()\n",
    "RNN_Model.add(word2vec_embedding_layer)\n",
    "RNN_Model.add(GRU(units=64,return_sequences=True,dropout=0.1))\n",
    "RNN_Model.add(GRU(units=32,dropout=0.1))\n",
    "RNN_Model.add(Dense(units=3,activation=\"softmax\"))\n",
    "RNN_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91075bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "418/418 [==============================] - 139s 323ms/step - loss: 0.6386 - accuracy: 0.3311\n",
      "Epoch 2/10\n",
      "418/418 [==============================] - 136s 326ms/step - loss: 0.6368 - accuracy: 0.3433\n",
      "Epoch 3/10\n",
      "418/418 [==============================] - 136s 326ms/step - loss: 0.6367 - accuracy: 0.3430\n",
      "Epoch 4/10\n",
      "418/418 [==============================] - 144s 345ms/step - loss: 0.6366 - accuracy: 0.3391\n",
      "Epoch 5/10\n",
      "418/418 [==============================] - 137s 328ms/step - loss: 0.6366 - accuracy: 0.3376\n",
      "Epoch 6/10\n",
      "418/418 [==============================] - 137s 328ms/step - loss: 0.6364 - accuracy: 0.3477\n",
      "Epoch 7/10\n",
      "418/418 [==============================] - 138s 329ms/step - loss: 0.6365 - accuracy: 0.3378\n",
      "Epoch 8/10\n",
      "418/418 [==============================] - 138s 330ms/step - loss: 0.6365 - accuracy: 0.3383\n",
      "Epoch 9/10\n",
      "418/418 [==============================] - 138s 330ms/step - loss: 0.6364 - accuracy: 0.3370\n",
      "Epoch 10/10\n",
      "418/418 [==============================] - 139s 333ms/step - loss: 0.6365 - accuracy: 0.3377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23fd62ab3a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "RNN_Model.fit(X_train_pad, y_train_dummy,batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "72fce18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 3s 19ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      1.00      0.48       472\n",
      "           1       0.00      0.00      0.00       508\n",
      "           2       0.00      0.00      0.00       507\n",
      "\n",
      "    accuracy                           0.32      1487\n",
      "   macro avg       0.11      0.33      0.16      1487\n",
      "weighted avg       0.10      0.32      0.15      1487\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ana\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\ana\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\ana\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "output_rnn = RNN_Model.predict(np.array(X_test))\n",
    "output_rnn = np.array(output_rnn).argmax(axis=1)\n",
    "print(classification_report(y_test,output_rnn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718516b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with different padding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dad4725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba0402cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad_2 = pad_sequences(encoded_train_data, \n",
    "                        maxlen=max_length_2, \n",
    "                        padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d177888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad_2 = pad_sequences(encoded_test_data, \n",
    "                        maxlen=max_length_2, \n",
    "                        padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93fe85ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "418/418 [==============================] - 38s 84ms/step - loss: 0.2265 - accuracy: 0.8594\n",
      "Epoch 2/5\n",
      "418/418 [==============================] - 36s 87ms/step - loss: 0.2185 - accuracy: 0.8650\n",
      "Epoch 3/5\n",
      "418/418 [==============================] - 32s 77ms/step - loss: 0.2144 - accuracy: 0.8693\n",
      "Epoch 4/5\n",
      "418/418 [==============================] - 36s 85ms/step - loss: 0.2183 - accuracy: 0.8645\n",
      "Epoch 5/5\n",
      "418/418 [==============================] - 35s 83ms/step - loss: 0.2073 - accuracy: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aeb12dc4c0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "RNN_Model.fit(X_train_pad_2, y_train_dummy,batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eebad78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 2s 22ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65       472\n",
      "           1       0.58      0.58      0.58       508\n",
      "           2       0.75      0.80      0.77       507\n",
      "\n",
      "    accuracy                           0.67      1487\n",
      "   macro avg       0.67      0.67      0.67      1487\n",
      "weighted avg       0.67      0.67      0.67      1487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = RNN_Model.predict(np.array(X_test_pad_2))\n",
    "output = np.array(output).argmax(axis=1)\n",
    "print(classification_report(y_test,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224326a7",
   "metadata": {},
   "source": [
    "# CNN W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88ef1e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 576, 100)          964500    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 576, 32)           9632      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 288, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               2304250   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 753       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,279,135\n",
      "Trainable params: 2,314,635\n",
      "Non-trainable params: 964,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model = Sequential()      # initilaizing the Sequential nature for CNN model\n",
    "# Adding the embedding layer which will take in maximum of 450 words as input and provide a 32 dimensional output of those words which belong in the top_words dictionary\n",
    "CNN_model.add(word2vec_embedding_layer)\n",
    "CNN_model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "CNN_model.add(Dropout(0.5))\n",
    "CNN_model.add(MaxPooling1D())\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(250, activation='relu'))\n",
    "CNN_model.add(Dropout(0.5))\n",
    "CNN_model.add(Dense(3, activation='softmax'))\n",
    "CNN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99862ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2682 - accuracy: 0.8218\n",
      "Epoch 2/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2691 - accuracy: 0.8238\n",
      "Epoch 3/10\n",
      "418/418 [==============================] - 18s 42ms/step - loss: 0.2661 - accuracy: 0.8267\n",
      "Epoch 4/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2632 - accuracy: 0.8309\n",
      "Epoch 5/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2639 - accuracy: 0.8264\n",
      "Epoch 6/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2643 - accuracy: 0.8319\n",
      "Epoch 7/10\n",
      "418/418 [==============================] - 18s 42ms/step - loss: 0.2670 - accuracy: 0.8268\n",
      "Epoch 8/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2660 - accuracy: 0.8246\n",
      "Epoch 9/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2591 - accuracy: 0.8300\n",
      "Epoch 10/10\n",
      "418/418 [==============================] - 18s 43ms/step - loss: 0.2600 - accuracy: 0.8290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7573d3f70>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.fit(X_train_pad, y_train_dummy,batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44793ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 92.4187\n",
      "Test Accuracy: 65.7700\n"
     ]
    }
   ],
   "source": [
    "# Report the accuracy scores for the training and test data.\n",
    "loss_train, acc_train = CNN_model.evaluate(X_train_pad, y_train_dummy, verbose=0)\n",
    "print('Train Accuracy: %.4f' % (acc_train*100))\n",
    "\n",
    "loss, acc = CNN_model.evaluate(X_test_pad, y_test_dummy, verbose=0)\n",
    "print('Test Accuracy: %.4f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "541f0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "output_cnn = CNN_model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c016e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cnn = np.array(output_cnn).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e63cd2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.58      0.63       472\n",
      "           1       0.52      0.69      0.59       508\n",
      "           2       0.83      0.71      0.76       507\n",
      "\n",
      "    accuracy                           0.66      1487\n",
      "   macro avg       0.68      0.66      0.66      1487\n",
      "weighted avg       0.68      0.66      0.66      1487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,output_cnn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
